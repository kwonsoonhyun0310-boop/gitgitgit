{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  ì£¼íƒì²­ì•½ FAQ ì‹œìŠ¤í…œ ì±—ë´‡ êµ¬í˜„ \n",
    "\n",
    "- ë¬¸ì„œ ì „ì²˜ë¦¬ + RAG + Gradio ChatInterface\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# í™˜ê²½ ì„¤ì • ë° ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) Env í™˜ê²½ë³€ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) LLM ì„¤ì •`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.4.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n",
      "    await self.dispatch_shell(msg, subshell_id=subshell_id)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/p7/4fhnsn7539q101qzrnk7dpkr0000gn/T/ipykernel_66223/1652494896.py\", line 1, in <module>\n",
      "    from langchain_openai import ChatOpenAI\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_openai/__init__.py\", line 3, in <module>\n",
      "    from langchain_openai.chat_models import AzureChatOpenAI, ChatOpenAI\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/__init__.py\", line 3, in <module>\n",
      "    from langchain_openai.chat_models.azure import AzureChatOpenAI\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_openai/chat_models/azure.py\", line 11, in <module>\n",
      "    from langchain_core.language_models import LanguageModelInput\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_core/language_models/__init__.py\", line 110, in __getattr__\n",
      "    result = import_attr(attr_name, module_name, __spec__.parent)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_core/_import_utils.py\", line 35, in import_attr\n",
      "    module = import_module(f\".{module_name}\", package=package)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/langchain_core/language_models/base.py\", line 41, in <module>\n",
      "    from transformers import GPT2TokenizerFast  # type: ignore[import-not-found]\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/transformers/__init__.py\", line 27, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/transformers/utils/__init__.py\", line 24, in <module>\n",
      "    from .auto_docstring import (\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/transformers/utils/auto_docstring.py\", line 30, in <module>\n",
      "    from .generic import ModelOutput\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/transformers/utils/generic.py\", line 51, in <module>\n",
      "    import torch\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/kwonsoonhyun/Sesac/004_llm_agent/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model='gpt-4.1-nano',\n",
    "    temperature=0.1,\n",
    "    top_p=0.9, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ë¬¸ì„œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ë¬¸ì„œ ë¡œë“œ\n",
    "\n",
    "- êµ­í† êµí†µë¶€ ì£¼íƒì²­ì•½ FAQì—ì„œ ì¼ë¶€ ë‚´ìš©(ì²­ì•½ìê²©, ì²­ì•½í†µì¥)ì„ ë°œì·Œí•˜ì—¬ ì¬ê°€ê³µ\n",
    "- Q1 ~ Q50ê¹Œì§€ ëª¨ë‘ 50ê°œì˜ ë¬¸ë‹µì´ í¬í•¨ëœ í…ìŠ¤íŠ¸ íŒŒì¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. \n",
      "ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n",
      "\n",
      "Q2 í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë‹¤ë©´ ì²­ì•½ì‹ ì²­ì´ ë¶ˆê°€ëŠ¥í•œì§€?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë”ë¼ë„ ì²­ì•½ê°€ëŠ¥ì§€ì—­ì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì— ì²­ì•½ì‹ ì²­ì´ ê°€ëŠ¥í•˜ë‚˜, ê°™ì€ ìˆœìœ„ì—ì„œëŠ” í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ê±°ì£¼ìê°€ ìš°ì„ í•˜ì—¬ ì£¼íƒì„ ê³µê¸‰ë°›ê²Œ ë©ë‹ˆë‹¤.\n",
      "* ì„œìš¸Â·ì¸ì²œÂ·ê²½ê¸°ë„ / ëŒ€ì „Â·ì„¸ì¢…Â·ì¶©ë‚¨ / ì¶©ë¶ / ê´‘ì£¼Â·ì „ë‚¨ / ì „ë¶ / ëŒ€êµ¬Â·ê²½ë¶ / ë¶€ì‚°Â·ìš¸ì‚°Â·ê²½ë‚¨ / ê°•ì›\n",
      "ë‹¤ë§Œ, ìˆ˜ë„ê¶Œ ëŒ€ê·œëª¨ íƒì§€ê°œë°œì§€êµ¬ ë“±ì—ì„œ ì£¼íƒì´ ê³µê¸‰ë˜ëŠ” ê²½ìš° ì¼ì • ë¹„ìœ¨ì˜ \n"
     ]
    }
   ],
   "source": [
    "# íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
    "faq_text_file = \"data/housing_faq.txt\"\n",
    "\n",
    "# íŒŒì¼ ì½ê¸° - íŒŒì´ì¬ ë‚´ì¥ í•¨ìˆ˜ ì‚¬ìš©\n",
    "with open(faq_text_file, 'r') as f:\n",
    "    faq_text = f.read()\n",
    "\n",
    "# íŒŒì¼ ë‚´ìš© í™•ì¸\n",
    "print(faq_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] TextLoaderë¥¼ ì‚¬ìš©í•˜ì—¬, í…ìŠ¤íŠ¸ ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ FAQ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œ\n",
    "loader = TextLoader(faq_text_file)  \n",
    "docs = loader.load()\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ FAQ í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë¡œë“œ\n",
    "loader = TextLoader(faq_text_file)  \n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1 ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. \n",
      "ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n",
      "\n",
      "Q2 í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë‹¤ë©´ ì²­ì•½ì‹ ì²­ì´ ë¶ˆê°€ëŠ¥í•œì§€?\n",
      "A í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— ê±°ì£¼í•˜ê³  ìˆì§€ ì•Šë”ë¼ë„ ì²­ì•½ê°€ëŠ¥ì§€ì—­ì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì— ì²­ì•½ì‹ ì²­ì´ ê°€ëŠ¥í•˜ë‚˜, ê°™ì€ ìˆœìœ„ì—ì„œëŠ” í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ê±°ì£¼ìê°€ ìš°ì„ í•˜ì—¬ ì£¼íƒì„ ê³µê¸‰ë°›ê²Œ ë©ë‹ˆë‹¤.\n",
      "* ì„œìš¸Â·ì¸ì²œÂ·ê²½ê¸°ë„ / ëŒ€ì „Â·ì„¸ì¢…Â·ì¶©ë‚¨ / ì¶©ë¶ / ê´‘ì£¼Â·ì „ë‚¨ / ì „ë¶ / ëŒ€êµ¬Â·ê²½ë¶ / ë¶€ì‚°Â·ìš¸ì‚°Â·ê²½ë‚¨ / ê°•ì›\n",
      "ë‹¤ë§Œ, ìˆ˜ë„ê¶Œ ëŒ€ê·œëª¨ íƒì§€ê°œë°œì§€êµ¬ ë“±ì—ì„œ ì£¼íƒì´ ê³µê¸‰ë˜ëŠ” ê²½ìš° ì¼ì • ë¹„ìœ¨ì˜ \n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'data/housing_faq.txt'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ë¬¸ì„œ ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê° ì§ˆë¬¸ê³¼ ë‹µë³€ì„ ìŒìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ì •ë¦¬ (ì •ê·œí‘œí˜„ì‹ í™œìš©)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_qa_pairs(text):\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # í…ìŠ¤íŠ¸ë¥¼ ë¼ì¸ë³„ë¡œ ë¶„ë¦¬í•˜ê³  ê° ë¼ì¸ì˜ ì•ë’¤ ê³µë°± ì œê±°\n",
    "    lines = [line.strip() for line in text.split('\\n')]\n",
    "    current_question = None\n",
    "    current_answer = []\n",
    "    current_number = None\n",
    "    in_answer = False\n",
    "    \n",
    "    for i, line in enumerate(lines):\n",
    "        if not line:  # ë¹ˆ ë¼ì¸ ì²˜ë¦¬\n",
    "            if in_answer and current_answer and i + 1 < len(lines) and lines[i + 1].startswith('Q'):\n",
    "                # ë‹¤ìŒ ì§ˆë¬¸ì´ ì‹œì‘ë˜ê¸° ì „ ë¹ˆ ì¤„ì´ë©´ í˜„ì¬ QA ìŒ ì €ì¥\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "                in_answer = False\n",
    "                current_answer = []\n",
    "            continue\n",
    "            \n",
    "        # ìƒˆë¡œìš´ ì§ˆë¬¸ í™•ì¸ (Q ë‹¤ìŒì— ìˆ«ìê°€ ì˜¤ëŠ” íŒ¨í„´)\n",
    "        q_match = re.match(r'Q(\\d+)\\s+(.*)', line)\n",
    "        if q_match:\n",
    "            # ì´ì „ QA ìŒì´ ìˆìœ¼ë©´ ì €ì¥\n",
    "            if current_question is not None and current_answer:\n",
    "                qa_pairs.append({\n",
    "                    'number': current_number,\n",
    "                    'question': current_question,\n",
    "                    'answer': ' '.join(current_answer).strip()\n",
    "                })\n",
    "            \n",
    "            # ìƒˆë¡œìš´ ì§ˆë¬¸ ì‹œì‘\n",
    "            current_number = int(q_match.group(1))\n",
    "            current_question = q_match.group(2).strip().rstrip('?') + '?'  # ì§ˆë¬¸ ë§ˆí¬ ì •ê·œí™”\n",
    "            current_answer = []\n",
    "            in_answer = False\n",
    "            \n",
    "        # ë‹µë³€ ì‹œì‘ í™•ì¸\n",
    "        elif line.startswith('A ') or (current_question and not current_answer and line):\n",
    "            in_answer = True\n",
    "            current_answer.append(line.lstrip('A '))\n",
    "            \n",
    "        # ê¸°ì¡´ ë‹µë³€ì— ë‚´ìš© ì¶”ê°€\n",
    "        elif current_question is not None and (in_answer or not line.startswith('Q')):\n",
    "            if in_answer or (current_answer and not line.startswith('Q')):\n",
    "                current_answer.append(line)\n",
    "    \n",
    "    # ë§ˆì§€ë§‰ QA ìŒ ì²˜ë¦¬\n",
    "    if current_question is not None and current_answer:\n",
    "        qa_pairs.append({\n",
    "            'number': current_number,\n",
    "            'question': current_question,\n",
    "            'answer': ' '.join(current_answer).strip()\n",
    "        })\n",
    "    \n",
    "    # ë²ˆí˜¸ ìˆœì„œëŒ€ë¡œ ì •ë ¬\n",
    "    qa_pairs.sort(key=lambda x: x['number'])\n",
    "    \n",
    "    return qa_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì¶”ì¶œëœ QA ìŒ ê°œìˆ˜: 50\n",
      "ì¶”ì¶œëœ ì²«ë²ˆì§¸ QA: \n",
      "{'number': 1, 'question': 'ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?', 'answer': 'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.'}\n"
     ]
    }
   ],
   "source": [
    "# QA ìŒ ì¶”ì¶œ\n",
    "qa_pairs = extract_qa_pairs(docs[0].page_content) \n",
    "\n",
    "print(f\"ì¶”ì¶œëœ QA ìŒ ê°œìˆ˜: {len(qa_pairs)}\")\n",
    "print(f\"ì¶”ì¶œëœ ì²«ë²ˆì§¸ QA: \\n{qa_pairs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) LLMìœ¼ë¡œ ì¶”ê°€ ì •ë³´ë¥¼ ì¶”ì¶œ`\n",
    "- í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œì™€ í•µì‹¬ ê°œë…ì„ ì¶”ì¶œí•˜ëŠ” ì²´ì¸\n",
    "- ë©”íƒ€ë°ì´í„° or ë³¸ë¬¸(page_content)ì— ì¶”ê°€í•˜ì—¬ ê²€ìƒ‰ì— í™œìš©    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] ë‹¤ìŒ ëª¨ë¸ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬, í‚¤ì›Œë“œì™€ ìš”ì•½ì„ ì¶”ì¶œí•˜ëŠ” ì²´ì¸ì„ ì™„ì„±í•©ë‹ˆë‹¤.*** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì˜ ë²”ìœ„ëŠ”?\n",
      "\n",
      "í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì´ë€ íŠ¹ë³„ì‹œã†ê´‘ì—­ì‹œã†íŠ¹ë³„ìì¹˜ì‹œã†íŠ¹ë³„ìì¹˜ë„(ê´€í•  êµ¬ì—­ ì•ˆì— ì§€ë°©ìì¹˜ë‹¨ì²´ì¸ ì‹œã†êµ°ì´ ì—†ëŠ” íŠ¹ë³„ìì¹˜ë„ë¥¼ ë§í•œë‹¤) ë˜ëŠ” ì‹œã†êµ°ì˜ í–‰ì •êµ¬ì—­ì„ ë§í•©ë‹ˆë‹¤. ë”°ë¼ì„œ, ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰í•˜ëŠ” ì£¼íƒì˜ ê²½ìš° ê³¼ì²œì‹œê°€ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤. ì°¸ê³ ë¡œ, ì„œìš¸íŠ¹ë³„ì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê²½ìš° ì„œìš¸íŠ¹ë³„ì‹œ ì „ì—­, ì¸ì²œê´‘ì—­ì‹œì˜ ê²½ìš° ì¸ì²œê´‘ì—­ì‹œ ì „ì—­ì´ í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­ì— í•´ë‹¹ë©ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# QA ìŒ í™•ì¸ (ì²«ë²ˆì§¸) ì§ˆë¬¸ ë‹µë³€ì´ ìˆëŠ”ë°, í•µì‹¬í‚¤ì›Œë“œ í•˜ë‚˜ ë½‘ì•„ë³¼ê±°ë‹¤ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìš”ì•½ì„ ë½‘ì•„ì˜¬ê±°ë‹¤.\n",
    "print((qa_pairs[0]['question']+\"\\n\\n\"+qa_pairs[0]['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì •ì˜ íŒŒì´ë´í‹± ë² ì´ìŠ¤ëª¨ë¸ ê¸°ë°˜ìœ¼ë¡œ ìë£Œ ì¶œë ¥êµ¬ì¡° \n",
    "class KeywordOutput(BaseModel):\n",
    "    keyword: str = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ(ë²•ë¥ ìš©ì–´, ì£¼ì œ ë“±))\")\n",
    "    summary: str = Field(description=\"í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "template = None\n",
    "\n",
    "# LCEL ì²´ì¸ êµ¬ì„± (Sturctured Output ì‚¬ìš©)\n",
    "prompt = None\n",
    "llm_with_structure = None \n",
    "keyowrd_extractor = None | None\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸     \n",
    "result = keyowrd_extractor.invoke(qa_pairs[0]['question']+qa_pairs[0]['answer'])\n",
    "print(\"í‚¤ì›Œë“œ:\", result.keywords)\n",
    "print(\"ìš”ì•½:\", result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì •ì˜\n",
    "class KeywordOutput(BaseModel):\n",
    "    keyword: str = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ(ë²•ë¥ ìš©ì–´, ì£¼ì œ ë“±))\")\n",
    "    summary: str = Field(description=\"í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½(í•œ ë¬¸ì¥)\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "template = \"\"\"ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½ì„ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í•µì‹¬ ìš©ì–´ë‚˜ ì „ë¬¸ ìš©ì–´, ì£¼ìš” ì•„ì´ë””ì–´ë‚˜ ì›ë¦¬ ë“±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "ì¶”ì¶œí•˜ëŠ” í‚¤ì›Œë“œëŠ” í•˜ë‚˜ë¡œ ì œí•œí•˜ë©°, í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{input_text}\n",
    "\n",
    "JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì‹œì˜¤:\n",
    "- keyword: í•µì‹¬ í‚¤ì›Œë“œ (1ê°œ)\n",
    "- summary: í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½ (1-2ë¬¸ì¥)\n",
    "\"\"\"\n",
    "\n",
    "# LCEL ì²´ì¸ êµ¬ì„±\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm_with_structure = llm.with_structured_output(KeywordOutput)  #llmì˜ ì¶œë ¥\n",
    "keyowrd_extractor = prompt | llm_with_structure\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸     \n",
    "result = keyowrd_extractor.invoke(qa_pairs[0]['question']+\"\\n\\n\"+qa_pairs[0]['answer'])\n",
    "print(\"í‚¤ì›Œë“œ:\", result.keyword)\n",
    "print(\"ìš”ì•½:\", result.summary)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í‚¤ì›Œë“œ: ì£¼íƒê±´ì„¤ì§€ì—­\n",
      "ìš”ì•½: ì´ í…ìŠ¤íŠ¸ëŠ” ê²½ê¸°ë„ ê³¼ì²œì‹œì—ì„œ ê³µê¸‰ë˜ëŠ” ì£¼íƒì˜ ê±´ì„¤ì§€ì—­ ë²”ìœ„ì™€ ê´€ë ¨ í–‰ì •êµ¬ì—­ì— ëŒ€í•´ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "\n",
    "# ì¶œë ¥ í˜•ì‹ ì •ì˜\n",
    "class KeywordOutput(BaseModel):\n",
    "    keyword: str = Field(description=\"í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œí•œ ê°€ì¥ ì¤‘ìš”í•œ í‚¤ì›Œë“œ(ë²•ë¥ ìš©ì–´, ì£¼ì œ ë“±))\")\n",
    "    summary: str = Field(description=\"í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½(í•œ ë¬¸ì¥)\")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜\n",
    "template = \"\"\"ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ì¤‘ìš”í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ê³ , í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½ì„ ì‘ì„±í•©ë‹ˆë‹¤.\n",
    "í…ìŠ¤íŠ¸ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì—¬ í•µì‹¬ ìš©ì–´ë‚˜ ì „ë¬¸ ìš©ì–´, ì£¼ìš” ì•„ì´ë””ì–´ë‚˜ ì›ë¦¬ ë“±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. \n",
    "ì¶”ì¶œí•˜ëŠ” í‚¤ì›Œë“œëŠ” í•˜ë‚˜ë¡œ ì œí•œí•˜ë©°, í…ìŠ¤íŠ¸ì˜ ì£¼ì œë¥¼ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "\n",
    "í…ìŠ¤íŠ¸:\n",
    "{input_text}\n",
    "\n",
    "JSON í˜•ì‹ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ë°˜í™˜í•˜ì‹œì˜¤:\n",
    "- keyword: í•µì‹¬ í‚¤ì›Œë“œ (1ê°œ)\n",
    "- summary: í…ìŠ¤íŠ¸ì˜ ê°„ë‹¨í•œ ìš”ì•½ (1-2ë¬¸ì¥)\n",
    "\"\"\"\n",
    "\n",
    "# LCEL ì²´ì¸ êµ¬ì„±\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "llm_with_structure = llm.with_structured_output(KeywordOutput)  #llmì˜ ì¶œë ¥\n",
    "keyowrd_extractor = prompt | llm_with_structure\n",
    "\n",
    "# í…ìŠ¤íŠ¸ ì¶”ì¶œ í…ŒìŠ¤íŠ¸     \n",
    "result = keyowrd_extractor.invoke(qa_pairs[0]['question']+\"\\n\\n\"+qa_pairs[0]['answer'])\n",
    "print(\"í‚¤ì›Œë“œ:\", result.keyword)\n",
    "print(\"ìš”ì•½:\", result.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) QA ìŒì„ ë¬¸ìì—´ í¬ë§·íŒ…í•˜ê³  ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document #ë°©ê¸ˆ í¬ë§¤íŒ…ëœ ë¬¸ìì—´ = ì§ˆë¬¸ë²ˆí˜¸, ì§ˆë¬¸ë‹µë³€ í˜ì´ì§€ ì»¨í…íŠ¸ë¥¼ ì¶”ê°€í•˜ê³  ìš”ì•½ë¬¸ ì¶”ê°€í•œê²ƒë“¤ì„ ë©”íƒ€ë°ì´í„°ë¡œ ë„£ëŠ”ë‹¤. \n",
    "#ê·¸ê±¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë‹´ì•„ì„œ 50ê°œì˜ ë°˜ë³µë¬¸ì´ ìˆë‹¤.\n",
    "\n",
    "def format_qa_pairs(qa_pairs):\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ QA ìŒì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "\n",
    "        # QA ìŒì„ í¬ë§·íŒ…\n",
    "        formatted_output = (\n",
    "            f\"[{pair['number']}]\\n\"\n",
    "            f\"ì§ˆë¬¸: {pair['question']}\\n\"\n",
    "            f\"ë‹µë³€: {pair['answer']}\\n\"\n",
    "        )\n",
    "\n",
    "        # í‚¤ì›Œë“œì™€ ìš”ì•½ ì¶”ì¶œ\n",
    "        result = keyowrd_extractor.invoke(pair['question']+\"\\n\\n\"+pair['answer'])\n",
    "\n",
    "        # ë¬¸ì„œ ê°ì²´ ìƒì„±\n",
    "        doc = Document(\n",
    "            page_content=formatted_output,\n",
    "            metadata={\n",
    "                'question_id': int(pair['number']),\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer'],\n",
    "                'keyword': result.keyword,\n",
    "                'summary': result.summary\n",
    "            }\n",
    "        )\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs\n",
    "\n",
    "\n",
    "# QA ìŒ í¬ë§·íŒ…\n",
    "formatted_docs = format_qa_pairs(qa_pairs)\n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œ ê°œìˆ˜: {len(formatted_docs)}\")\n",
    "\n",
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(formatted_docs[0].page_content)\n",
    "print(\"-\" * 200)\n",
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "pprint(formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì €ì¥\n",
    "output_file = \"data/housing_faq_formatted.json\"\n",
    "\n",
    "#ë­ì²´ì¸ ë‹¤íë¨¼íŠ¸ ë˜ìˆëŠ”ê±°ë¥¼ ì œì´ìŠ¨ìœ¼ë¡œ ë°”ê¾¸ëŠ” í˜•ì‹ì´ë¡œ ë¬¸ì„œë“¤ìœ¼ ì €ì¥í•œê±°ë‹¤.\n",
    "with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([doc.model_dump() for doc in formatted_docs], f, indent=2, ensure_ascii=False)  # í•œê¸€ì´ ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜ë˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œë¥¼ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] ë¬¸ì„œ ê°ì²´ë¥¼ í¬ë§·íŒ…í•˜ì—¬ êµ¬ì„±í•©ë‹ˆë‹¤.*** \n",
    "\n",
    "- ìš”ì•½ë¬¸ì„ ì‹œë§¨í‹± ê²€ìƒ‰ì— í™œìš©í•©ë‹ˆë‹¤. ë‹¤ìŒ êµ¬ì¡°ë¡œ ë¬¸ì„œ ê°ì²´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "    - page_content: ìš”ì•½\n",
    "    - metadata: ê¸°íƒ€ ì •ë³´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. ì´ê±´ ìš”ì•½ë¬¸ì„ ë‹¤ë¥´ê²Œ êµ¬ì„±í•´ë†“ì€ ì‹¤ìŠµì´ë‹¤.\n",
    "\n",
    "\n",
    "def format_qa_pairs_with_summary(qa_pairs):\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ QA ìŒì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def format_qa_pairs_with_summary(qa_pairs):\n",
    "    \"\"\"\n",
    "    ì¶”ì¶œëœ QA ìŒì„ í¬ë§·íŒ…í•˜ì—¬ ë¬¸ì„œ ê°ì²´ë¡œ ë³€í™˜\n",
    "    \"\"\"\n",
    "    processed_docs = []\n",
    "    for pair in qa_pairs:\n",
    "\n",
    "        # í‚¤ì›Œë“œì™€ ìš”ì•½ ì¶”ì¶œ\n",
    "        result = keyowrd_extractor.invoke(pair['question']+\"\\n\\n\"+pair['answer'])\n",
    "\n",
    "        # ë¬¸ì„œ ê°ì²´ ìƒì„±\n",
    "        doc = Document(\n",
    "            page_content=result.summary,\n",
    "            metadata={\n",
    "                'question_id': int(pair['number']),\n",
    "                'question': pair['question'],\n",
    "                'answer': pair['answer'],\n",
    "                'keyword': result.keyword,\n",
    "            }\n",
    "        )\n",
    "        processed_docs.append(doc)\n",
    "\n",
    "    return processed_docs\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA ìŒ í¬ë§·íŒ…\n",
    "summary_formatted_docs = format_qa_pairs_with_summary(qa_pairs) \n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œ ê°œìˆ˜: {len(summary_formatted_docs)}\")\n",
    "\n",
    "# ë¬¸ì„œ í™•ì¸\n",
    "print(summary_formatted_docs[0].page_content)\n",
    "print(\"-\" * 200)\n",
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "pprint(summary_formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ì €ì¥\n",
    "output_file = \"data/housing_faq_formatted_with_summary.json\"\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8-sig') as f:\n",
    "    json.dump([doc.model_dump() for doc in summary_formatted_docs], f, indent=2, ensure_ascii=False)  # í•œê¸€ì´ ìœ ë‹ˆì½”ë“œë¡œ ë³€í™˜ë˜ì§€ ì•Šë„ë¡ ì„¤ì •\n",
    "print(f\"í¬ë§·íŒ…ëœ ë¬¸ì„œë¥¼ {output_file}ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë²¡í„° ì €ì¥ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ ì œì´ìŠ¨í˜•ì‹ìœ¼ë¡œëœê±¸ ì½ì–´ì™€ì„œ ì´ê±´ ë¦¬ìŠ¤íŠ¸ë‹ˆê¹Œ í•˜ë‚˜ì˜ ì•„ì´í…œë“¤ì„ í˜ì´ì§€ì»¨í…íŠ¸ì†ì„±ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì†ì„±ì„ ì§€ì • í‚¤ë°¸ë¥˜ë¡œ ë˜ìˆëŠ”ê±°ë¥¼ ê°–ëŠ”ìƒíƒœë¡œ ë³€í™˜í•´ì¤€ë‹¤\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_file = \"data/housing_faq_formatted.json\"\n",
    "\n",
    "#ì œì´ìŠ¨ìœ¼ë¡œ ì €ì¥ëœê±° ë¶ˆëŸ¬ì˜¨ê±°ë‹¤ ì½ì–´ì˜¨ê±°ë‹¤.\n",
    "with open(output_file, 'r', encoding='utf-8-sig') as f:\n",
    "    formatted_docs = [Document(**doc) for doc in json.load(f)]\n",
    "\n",
    "print(len(formatted_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'formatted_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m embeddings = OpenAIEmbeddings(model=\u001b[33m\"\u001b[39m\u001b[33mtext-embedding-3-small\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# ë¬¸ì„œ ë²¡í„° ì €ì¥ í¬ë¡œë§ˆì— ì»¬ë ‰ì…˜ì´ë¦„ ìƒˆë¡œ ì§€ì •í•´ì„œ ì €ì¥ì„ í• ê±°ë‹¤. 50ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë ê±°ë‹¤.\u001b[39;00m\n\u001b[32m      7\u001b[39m vector_store = Chroma.from_documents(  \n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     documents=\u001b[43mformatted_docs\u001b[49m,\n\u001b[32m      9\u001b[39m     embedding=embeddings,\n\u001b[32m     10\u001b[39m     collection_name=\u001b[33m\"\u001b[39m\u001b[33mhousing_faq_db\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m     persist_directory=\u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'formatted_docs' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë¬¸ì„œ ë²¡í„° ì €ì¥ í¬ë¡œë§ˆì— ì»¬ë ‰ì…˜ì´ë¦„ ìƒˆë¡œ ì§€ì •í•´ì„œ ì €ì¥ì„ í• ê±°ë‹¤. 50ê°œì˜ ë¬¸ì„œê°€ ì €ì¥ë ê±°ë‹¤.\n",
    "vector_store = Chroma.from_documents(  \n",
    "    documents=formatted_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._collection.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] ìš”ì•½ ë¬¸ì„œ(summary_formatted_docs)ë¥¼ ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥í•©ë‹ˆë‹¤.*** \n",
    "\n",
    "- OpenAI (text-embedding-3-small) ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "- Chromda DB ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¬¸ì„œ ë¡œë“œ\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "output_file = \"data/housing_faq_formatted_with_summary.json\"\n",
    "\n",
    "with open(output_file, 'r', encoding='utf-8-sig') as f:\n",
    "    summary_formatted_docs = [Document(**doc) for doc in json.load(f)]\n",
    "\n",
    "# ë¬¸ì„œ í™•ì¸ ë™ì¼í•œ ë¬¸ì„œ ì½ì–´ì˜¬ìˆ˜ìˆê³ \n",
    "print(summary_formatted_docs[0].page_content)\n",
    "\n",
    "# ë¬¸ì„œ ë©”íƒ€ë°ì´í„° í™•ì¸\n",
    "pprint(summary_formatted_docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë¬¸ì„œ ë²¡í„° ì €ì¥\n",
    "vector_store_with_summary = Chroma.from_documents(  \n",
    "    documents=summary_formatted_docs,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"housing_faq_summary_db\",\n",
    "    persist_directory=\"./chroma_db\",\n",
    ")\n",
    "\n",
    "vector_store_with_summary._collection.count()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ë¬¸ì„œ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3ê°œì˜ ë¬¸ì„œì½ì–´ì˜¤ê²Œí•´ì„œ\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {vector_store._collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] ì•ì—ì„œ ì €ì¥í•œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.*** \n",
    "\n",
    "- OpenAI (text-embedding-3-small) ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©\n",
    "- Chromda DB ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_store_with_summary = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_store_with_summary = Chroma(\n",
    "    collection_name=\"housing_faq_summary_db\",\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "print(f\"ë²¡í„° ì €ì¥ì†Œì— ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜: {vector_store_with_summary._collection.count()}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë²¡í„° ì €ì¥ì†Œë¥¼ ë¦¬íŠ¸ë¦¬í„° ê²€ìƒ‰ë„êµ¬ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰ ë°”ë¡œì•„ë˜ ê·¸ëƒ¥ ë¦¬íŠ¸ë¦¬ë²„ë§Œë“¤ì–´ë†“ìŒ\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "#llmì´ ì´ ë‚´ìš©ì„ ê°€ì§€ê³ , ê³¼ì²œì‹œë‘ ìˆ˜ì›ì‹œë‘ 1ëŒ€1ëŒ€ì‘í•´ì„œ ëŒ€ë‹µí•˜ëŠ”ë°, ì´ê±¸ ì´í•´ëª»í•˜ë©´ ë‹µë³€ìƒì„±ì— ì‹¤íŒ¨í•œë‹¤.\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] MMR ê²€ìƒ‰ê¸°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.*** \n",
    "\n",
    "- ìš”ì•½ ë¬¸ì„œ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ì‚¬ìš©\n",
    "- 10ê°œì˜ ë¬¸ì„œë¥¼ ê°€ì ¸ì™€ì„œ, ë‹¤ì–‘ì„± ê¸°ë°˜ìœ¼ë¡œ 3ê°œë¥¼ ì„ íƒ (ë‹¤ì–‘ì„±ì€ ì¤‘ê°„ ìˆ˜ì¤€ ì ìš©)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "mmr_retriever = None\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = mmr_retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(result.metadata['question'])\n",
    "    print(result.metadata['answer'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "mmr_retriever = vector_store_with_summary.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={\"k\": 3, \"fetch_k\": 10, \"lambda_mult\": 0.5},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = mmr_retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(result.metadata['question'])\n",
    "    print(result.metadata['answer'])\n",
    "    print(\"=\" * 50)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[ì‹¬í™”] ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§**\n",
    "\n",
    "- Chroma ë¬¸ì„œ: https://docs.trychroma.com/docs/querying-collections/metadata-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ë‹¨ì¼ í•„ë“œ ì •í™•íˆ ì¼ì¹˜ #ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨. ì¤‘ìš”. ë©”íƒ€ë°ì´í„° ê¸¸ì´ì—ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ì–´ì„œ ì—…ë°ì´íŠ¸í•˜ë©´, ë˜‘ê°™ì€ ì¿¼ë¦¬ë¥¼ ë„£ì–´ë„ ê²€ìƒ‰ê²°ê³¼ê°€ ë‹¬ë¼ì§„ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì€ ì›”ë³„ë¡œ ë”°ë¡œ ë§Œë“ ë‹¤ë˜ì§€, ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë”°ë¡œ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤ë˜ì§€\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# ë²¡í„°ì €ì¥ì†Œê°€ ì—¬ëŸ¬êµ°ë° ë¶„ë¦¬ê°€ ë¼ì„ê±´ë° ì¿¼ë¦¬ê°€ ê²©ë¦¬ë¼ì„œ ì €ì¥í•˜ëŠ” ê¸°ë²• ì¤‘ìš”. í¬ë¡œë§ˆë‚˜ ì¼ë¼ì„œì¹˜ë¡œ ì €ì¥ì†Œ ë”°ë¡œë„ ìˆê³ , ë‚ ì§œ, ì¹´í…Œê³ ë¦¬í™”. ì˜ì •ë¦¬í•´ì„œ ë¼ìš°íŒ…ì„ ì˜ í•˜ë„ë¡í•´ì•¼í•œë‹¤.\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#ë©”íƒ€ë°ì´í„°ëŠ” ê²©ë¦¬ëŠ” ì•„ë‹ˆê³ , ê·¸ì•ˆì—ì„œ ë©”íƒ€ë°ì´í„°ì•ˆì—ì„œ ì—°ë„ë¼ë˜ì§€ ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ì •ë¦¬í•´ë‘ë©´ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ê²€ìƒ‰í•´ì˜¤ë©´ \u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#í•„í„°ë¥¼ ì§€ì›ì„ í•œë‹¤.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m retriever = \u001b[43mvector_store\u001b[49m.as_retriever(\n\u001b[32m      8\u001b[39m     search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mfilter\u001b[39m\u001b[33m\"\u001b[39m: {\u001b[33m\"\u001b[39m\u001b[33mkeyword\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mí•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\u001b[39m\u001b[33m\"\u001b[39m}},\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     13\u001b[39m results = retriever.invoke(query)\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì¼ í•„ë“œ ì •í™•íˆ ì¼ì¹˜ #ì‹¤ë¬´ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë¨. ì¤‘ìš”. ë©”íƒ€ë°ì´í„° ê¸¸ì´ì—ë”°ë¼ ë‹¬ë¼ì§„ë‹¤.\n",
    "#ì»¨í…ìŠ¤íŠ¸ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ë„£ì–´ì„œ ì—…ë°ì´íŠ¸í•˜ë©´, ë˜‘ê°™ì€ ì¿¼ë¦¬ë¥¼ ë„£ì–´ë„ ê²€ìƒ‰ê²°ê³¼ê°€ ë‹¬ë¼ì§„ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ë°©ë²•ì€ ì›”ë³„ë¡œ ë”°ë¡œ ë§Œë“ ë‹¤ë˜ì§€, ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë”°ë¡œ ë§Œë“¤ì–´ë†“ëŠ”ë‹¤ë˜ì§€\n",
    "# ë²¡í„°ì €ì¥ì†Œê°€ ì—¬ëŸ¬êµ°ë° ë¶„ë¦¬ê°€ ë¼ì„ê±´ë° ì¿¼ë¦¬ê°€ ê²©ë¦¬ë¼ì„œ ì €ì¥í•˜ëŠ” ê¸°ë²• ì¤‘ìš”. í¬ë¡œë§ˆë‚˜ ì¼ë¼ì„œì¹˜ë¡œ ì €ì¥ì†Œ ë”°ë¡œë„ ìˆê³ , ë‚ ì§œ, ì¹´í…Œê³ ë¦¬í™”. ì˜ì •ë¦¬í•´ì„œ ë¼ìš°íŒ…ì„ ì˜ í•˜ë„ë¡í•´ì•¼í•œë‹¤.\n",
    "\n",
    "#ë©”íƒ€ë°ì´í„°ëŠ” ê²©ë¦¬ëŠ” ì•„ë‹ˆê³ , ê·¸ì•ˆì—ì„œ ë©”íƒ€ë°ì´í„°ì•ˆì—ì„œ ì—°ë„ë¼ë˜ì§€ ì¹´í…Œê³ ë¦¬ë§ˆë‹¤ ì •ë¦¬í•´ë‘ë©´ ì¹´í…Œê³ ë¦¬ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ ê²€ìƒ‰í•´ì˜¤ë©´ \n",
    "#í•„í„°ë¥¼ ì§€ì›ì„ í•œë‹¤.\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $eq ì—°ì‚°ì ì‚¬ìš© - ì •í™•íˆ ì¼ì¹˜, ë‘ë²ˆì§¸ë°©ë²•ì€ ì´í€„ì—°ì‚°ì ì‚¬ìš©í•œë‹¤. í¬ë¡œë§ˆ ë²¡í„°ë””ë¹„ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ê²€ìƒ‰í•´ì™€ì„œ í•„í„°ë¥¼ í•œë‹¤. ë„ˆë¬´ì»¤ì§„ë‹¤ì‹¶ìœ¼ë©´ ë””ë¹„ë¥¼ ì—¬ëŸ¬ê°œë¡œ ë¶„í• í•œë‹¤.\n",
    "#ë©”íƒ€ë°ì´í„° í•„í„°ë§ì€ ì „ì²´ê²€ìƒ‰í•´ì„œ í•„í„°í•˜ëŠ”ë°, ì• ì´ˆì— ë‚˜ë‰œê±°ë§Œ ê²€ìƒ‰í•˜ëŠ” ê²ƒë„ ê°œë°œì¤‘ì´ë‹¤.\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$eq\": \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ne (Not Equal) ì—°ì‚°ì ì‚¬ìš© - ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ” ë¬¸ì„œ ê²€ìƒ‰, ì´ë°©ë²•ì€ ì•„ë‹Œ ë¬¸ì„œë§Œ ê²€ìƒ‰í•˜ëŠ”ê±°ë‹¤.\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$ne\": \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $in ì—°ì‚°ìë¡œ ì—¬ëŸ¬ ê°’ ì¤‘ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰ \n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"keyword\": {\"$in\": [\"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\", \"ì²­ì•½ì˜ˆê¸ˆ\"]}}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ«ì ë²”ìœ„ ê²€ìƒ‰ ($gt, $gte, $lt, $lte) - question_idê°€ 10 ì´ìƒì¸ ë¬¸ì„œ ê²€ìƒ‰ \n",
    "#ìˆ«ìë¡œëœê±°ëŠ” ë²”ìœ„ì§€ì •ì„ í•  ìˆ˜ê°€ ìˆë‹¤.\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"question_id\": {\"$gte\": 10}}},\n",
    ")\n",
    "\n",
    "query = \"ë¬´ì£¼íƒì ê¸°ì¤€ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $andë¡œ ì—¬ëŸ¬ ì¡°ê±´ ì¡°í•© - keywordê°€ \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ê³  question_idê°€ 10 ë¯¸ë§Œì¸ ë¬¸ì„œ ê²€ìƒ‰\n",
    "#ì§ˆë¬¸ë²ˆí˜¸ê°€ 10ë³´ë‹¤ ì‘ì€ê±°\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$and\": [\n",
    "        {\"keyword\": \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"}, \n",
    "        {\"question_id\": {\"$lt\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $orë¡œ ì—¬ëŸ¬ ì¡°ê±´ ì¤‘ í•˜ë‚˜ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰ - keywordê°€ \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ê±°ë‚˜ question_idê°€ 10 ì´ìƒì¸ ë¬¸ì„œ ê²€ìƒ‰\n",
    "#orì€ ë‘˜ì¤‘ì— í•˜ë‚˜ë¼ë„ í¬í•¨ë¼ì‡ëŠ”ê±°\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"filter\": {\"$or\": [\n",
    "        {\"keyword\": \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\"}, \n",
    "        {\"question_id\": {\"$gte\": 10}}\n",
    "    ]}},\n",
    ")\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ê·œì‹ íŒ¨í„´ ë§¤ì¹­ - page_content ë³¸ë¬¸ì— \"ì£¼íƒê±´ì„¤ì§€ì—­\"ì´ í¬í•¨ëœ ë¬¸ì„œ ê²€ìƒ‰\n",
    "\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={'where_document': {'$contains': 'ì£¼íƒê±´ì„¤ì§€ì—­'}},\n",
    ") #ë¬¸ìì—´ì´ í¬í•¨ë˜ëŠ” ì§€ì—­ì„ ì²´í¬í•´ì£¼ëŠ” ë³¸ë¬¸ê²€ìƒ‰ê¸°ëŠ¥ë„ ë¶€ë¶„ì ìœ¼ë¡œëŠ” ì ìš©í•œë‹¤.\n",
    "\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(result.metadata['question_id'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ë©”íƒ€ë°ì´í„° í•„í„°ë§ ì²´ì¸ (`lark` íŒ¨í‚¤ì§€ ì„¤ì¹˜ í•„ìš”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ğŸš€ ê²€ìƒ‰ ì‹œì‘: 'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\n",
      "ğŸ”„ LLM ê¸°ë°˜ ë¶„ì„ ì‹œë„...\n",
      "âš ï¸ LLM ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "ğŸ”„ ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰...\n",
      "âŒ ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 'ì²­ì•½ì˜ˆê¸ˆ' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë“¤\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ğŸš€ ê²€ìƒ‰ ì‹œì‘: 'ì²­ì•½ì˜ˆê¸ˆ' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë“¤\n",
      "ğŸ”„ LLM ê¸°ë°˜ ë¶„ì„ ì‹œë„...\n",
      "âš ï¸ LLM ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "ğŸ”„ ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰...\n",
      "âŒ ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: 'ì§ˆë¬¸ ID'ê°€ 5ì—ì„œ 15 ì‚¬ì´ì¸ ë¬¸ì„œ\n",
      "----------------------------------------------------------------------------------------------------\n",
      "ğŸš€ ê²€ìƒ‰ ì‹œì‘: 'ì§ˆë¬¸ ID'ê°€ 5ì—ì„œ 15 ì‚¬ì´ì¸ ë¬¸ì„œ\n",
      "ğŸ”„ LLM ê¸°ë°˜ ë¶„ì„ ì‹œë„...\n",
      "âš ï¸ LLM ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "ğŸ”„ ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰...\n",
      "âŒ ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨: name 'vector_store' is not defined\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import chain\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict, Any, Union\n",
    "import re\n",
    "import json\n",
    "\n",
    "#ì§ˆë¬¸í•˜ìë§ˆì ì§ˆë¬¸í•˜ëŠ”ê±°ë¥¼ í•„í„°ë¡œ ë‚˜ëˆ ì£¼ëŠ” ì—­í• ì„ ë§Œë“¤ê³ ì‹¶ì—‡ë‹¤.\n",
    "\n",
    "class QueryFilter(BaseModel):\n",
    "    \"\"\"ì¿¼ë¦¬ í•„í„° ì •ë³´ë¥¼ ë‹´ëŠ” ëª¨ë¸\"\"\"\n",
    "    semantic_query: str = Field(description=\"ì˜ë¯¸ ê²€ìƒ‰ì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸\")\n",
    "    keyword: Optional[str] = Field(default=None, description=\"í‚¤ì›Œë“œ í•„í„°\")\n",
    "    question_id_min: Optional[int] = Field(default=None, description=\"question_id ìµœì†Œê°’\")\n",
    "    question_id_max: Optional[int] = Field(default=None, description=\"question_id ìµœëŒ€ê°’\")\n",
    "    limit: Optional[int] = Field(default=None, description=\"ê²°ê³¼ ê°œìˆ˜ ì œí•œ\") #k ê°’ ì •í•˜ë ¤ê³  ë§Œë“¤ì–´ë†“ìŒ\n",
    "\n",
    "class CustomQueryRetriever:\n",
    "    \"\"\"\n",
    "    SelfQueryRetrieverë¥¼ ëŒ€ì²´í•˜ëŠ” ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„\n",
    "    LLMìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ì§ì ‘ Chroma í•„í„°ë¥¼ ìƒì„±í•œë‹¤\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vectorstore, llm=None):\n",
    "        self.vectorstore = vectorstore\n",
    "        self.llm = llm or ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "        self.query_parser = self._create_query_parser()\n",
    "    \n",
    "    def _create_query_parser(self):\n",
    "        \"\"\"êµ¬ì¡°í™”ëœ ì¿¼ë¦¬ íŒŒì„œ ìƒì„±\"\"\"#ìµœì¢…ì¤„ë ¥ì€ ì¿¼ë¦¬ íŒŒì„œí•„í„°\n",
    "        \n",
    "        system_prompt = \"\"\"ì‚¬ìš©ìì˜ ìì—°ì–´ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ ê²€ìƒ‰ì— í•„ìš”í•œ ì •ë³´ë¥¼ êµ¬ì¡°í™”í•œë‹¤.\n",
    "\n",
    "ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¥¸ë‹¤:\n",
    "1. semantic_query: ì˜ë¯¸ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œë‚˜ ë‚´ìš©\n",
    "2. keyword: ì •í™•íˆ ì¼ì¹˜í•´ì•¼ í•˜ëŠ” í‚¤ì›Œë“œ (ìˆëŠ” ê²½ìš°ë§Œ)\n",
    "3. question_id_min/max: ì§ˆë¬¸ ID ë²”ìœ„ (ì˜ˆ: \"10ë²ˆ ì´í•˜\" â†’ max=10, \"5ë²ˆ ì´ìƒ\" â†’ min=5)\n",
    "4. limit: ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜ ì œí•œ (ê¸°ë³¸ê°’ì€ 5ê°œ)\n",
    "\n",
    "ì˜ˆì‹œ:\n",
    "- \"'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­' ê´€ë ¨ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\"\n",
    "  â†’ semantic_query: \"ì£¼íƒê±´ì„¤ì§€ì—­\", keyword: \"í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­\", question_id_max: 10\n",
    "\n",
    "- \"'ì£¼íƒê³µê¸‰' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ 5ê°œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\"\n",
    "  â†’ semantic_query: \"ì£¼íƒê³µê¸‰\", keyword: \"ì£¼íƒê³µê¸‰\", limit: 5\n",
    "\n",
    "ì •ë³´ê°€ ì—†ëŠ” í•„ë“œëŠ” nullë¡œ ì„¤ì •í•œë‹¤.\"\"\"\n",
    "\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"ë‹¤ìŒ ì¿¼ë¦¬ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”: {query}\")\n",
    "        ]) #ì‹œìŠ¤í…œí”„ë¡¬í”„íŠ¸ì— ìì„¸íˆ ë‹¤ ì”€\n",
    "        \n",
    "        return prompt | self.llm.with_structured_output(QueryFilter)\n",
    "    \n",
    "    def _build_chroma_filter(self, filter_info: QueryFilter) -> Dict[str, Any]:\n",
    "        \"\"\"QueryFilter ì •ë³´ë¥¼ Chroma DB í•„í„°ë¡œ ë³€í™˜\"\"\"\n",
    "        conditions = []\n",
    "        \n",
    "        # í‚¤ì›Œë“œ í•„í„°\n",
    "        if filter_info.keyword:\n",
    "            conditions.append({\"keyword\": {\"$eq\": filter_info.keyword}})\n",
    "        \n",
    "        # question_id ë²”ìœ„ í•„í„°\n",
    "        if filter_info.question_id_min is not None and filter_info.question_id_max is not None:\n",
    "            conditions.append({\n",
    "                \"$and\": [\n",
    "                    {\"question_id\": {\"$gte\": filter_info.question_id_min}},\n",
    "                    {\"question_id\": {\"$lte\": filter_info.question_id_max}}\n",
    "                ]\n",
    "            }) #ìµœì†Ÿê°’ë§Œ ìˆìœ¼ë©´ ì´ìƒ ìµœëŒ“ê°“ë§Œì‡ìœ¼ë©´ ì´í•˜ë¡œ\n",
    "        elif filter_info.question_id_min is not None:\n",
    "            conditions.append({\"question_id\": {\"$gte\": filter_info.question_id_min}})\n",
    "        elif filter_info.question_id_max is not None:\n",
    "            conditions.append({\"question_id\": {\"$lte\": filter_info.question_id_max}})\n",
    "        \n",
    "        # ì¡°ê±´ë“¤ì„ ANDë¡œ ê²°í•©\n",
    "        if len(conditions) == 0:\n",
    "            return {}\n",
    "        elif len(conditions) == 1:\n",
    "            return conditions[0]\n",
    "        else:\n",
    "            return {\"$and\": conditions}\n",
    "    \n",
    "    def invoke(self, query: str) -> List[Document]:\n",
    "        \"\"\"ì¿¼ë¦¬ ì‹¤í–‰\"\"\" #ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„ë¡œ ë§Œë“œë €ê³  ì—¬ê¸° ë¦¬íŠ¸ë¦¬ë²„ ë¶€ë¶„ ì¸ë³´í¬ë¡œ ì‹¤í–‰í•˜ë©´\n",
    "        try:\n",
    "            # ì¿¼ë¦¬ ë¶„ì„ ì¿¼ë¦¬ê°€ë“¤ì–´ì˜¤ë©´ í•„í„°ì •ë³´ë¥¼ ì¶”ì¶œ ì¶”ì¶œëœê±°ë¡œ ê·¸ë¡œë§ˆí•„í„° ìƒì„±\n",
    "            filter_info = self.query_parser.invoke({\"query\": query})\n",
    "            print(f\"ğŸ” ë¶„ì„ëœ ì¿¼ë¦¬: {filter_info}\")\n",
    "            \n",
    "            # Chroma í•„í„° ìƒì„±\n",
    "            chroma_filter = self._build_chroma_filter(filter_info)\n",
    "            print(f\"ğŸ”§ ìƒì„±ëœ í•„í„°: {chroma_filter}\")\n",
    "            \n",
    "            # ê²€ìƒ‰ ì‹¤í–‰\n",
    "            search_kwargs = {}\n",
    "            \n",
    "            if chroma_filter:\n",
    "                search_kwargs[\"filter\"] = chroma_filter\n",
    "            \n",
    "            k = filter_info.limit or 5 #ë¦¬ë°‹ì„ ì§€ì •\n",
    "            search_kwargs[\"k\"] = k\n",
    "            \n",
    "            # ì˜ë¯¸ ê²€ìƒ‰ ì¿¼ë¦¬ ê²°ì •\n",
    "            search_query = filter_info.semantic_query or query\n",
    "            #ë²¡í„°ìŠ¤í† ì–´ì— ìœ ì‚¬ë„ê²€ìƒ‰ì‚¬ìš©\n",
    "            results = self.vectorstore.similarity_search(\n",
    "                query=search_query,\n",
    "                **search_kwargs\n",
    "            ) #llmì´ ìƒì„±í•˜ë„ë¡ ë§Œë“¬\n",
    "            \n",
    "            print(f\"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ë¬¸ì„œ ë°œê²¬\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}\")\n",
    "            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ê²€ìƒ‰\n",
    "            return self.vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "\n",
    "\n",
    "@chain #ì¿¼ë¦¬ê°€ ë“¤ì–´ì˜¤ë©´ ë¦¬íŠ¸ë¦¬ë²„ì²´ì¸ì´ê³  ë¬¸ì„œê°€ ê²€ìƒ‰ëœê²°ê³¼ê°€ ë¦¬í„´ì´ë˜ê³  \n",
    "def smart_custom_search(query: str) -> List[Document]:\n",
    "    \"\"\"\n",
    "    ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì‚¬ìš©í•œ ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰\n",
    "    \"\"\"\n",
    "    print(f\"ğŸš€ ê²€ìƒ‰ ì‹œì‘: {query}\")\n",
    "    \n",
    "    # 1ì°¨ ì‹œë„: LLM ê¸°ë°˜ ì»¤ìŠ¤í…€ ë¦¬íŠ¸ë¦¬ë²„\n",
    "    try:\n",
    "        print(\"ğŸ”„ LLM ê¸°ë°˜ ë¶„ì„ ì‹œë„...\") #ì•ì— ì»¤ìŠ¤í…€ë¦¬íŠ¸ë¦¬ë²„ë§Œë“ ê±¸í†µí•´ì„œ ê²€ìƒ‰í•˜ë©´ ê·¸ê²°ê³¼ê°€ ë¦¬í„´í•´ì£¼ê²Œë§Œë“¬\n",
    "        llm = ChatOpenAI(model='gpt-4.1-mini', temperature=0)\n",
    "        retriever = CustomQueryRetriever(vector_store, llm)\n",
    "        results = retriever.invoke(query)\n",
    "        if results:\n",
    "            return results #ì‹¤í–‰ì¤‘ì— ì—ëŸ¬ë°œìƒí•˜ë©´\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ LLM ê¸°ë°˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    # 2ì°¨ ì‹œë„: ê¸°ë³¸ ê²€ìƒ‰ #ì¼ë°˜ê²€ìƒ‰ìœ¼ë¡œ\n",
    "    try:\n",
    "        print(\"ğŸ”„ ê¸°ë³¸ ê²€ìƒ‰ ì‹¤í–‰...\")\n",
    "        results = vector_store.similarity_search(query, k=5)\n",
    "        print(f\"âœ… ê¸°ë³¸ ê²€ìƒ‰ ì„±ê³µ: {len(results)}ê°œ ë¬¸ì„œ\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ëª¨ë“  ê²€ìƒ‰ ì‹¤íŒ¨: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸ #í…ŒìŠ¤íŠ¸ì§ˆë¬¸\n",
    "test_queries = [\n",
    "    \"'í•´ë‹¹ ì£¼íƒê±´ì„¤ì§€ì—­' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë¥¼ 10ë²ˆ ì´í•˜ì¸ ë¬¸ì„œì¤‘ì—ì„œ ê²€ìƒ‰í•´ì£¼ì„¸ìš”\",\n",
    "    \"'ì²­ì•½ì˜ˆê¸ˆ' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œë“¤\",\n",
    "    \"'ì§ˆë¬¸ ID'ê°€ 5ì—ì„œ 15 ì‚¬ì´ì¸ ë¬¸ì„œ\",\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nğŸ“ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {query}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    try:\n",
    "        # ì»¤ìŠ¤í…€ ê²€ìƒ‰ ì‹¤í–‰\n",
    "        results = smart_custom_search.invoke(query)\n",
    "        \n",
    "        # ê²°ê³¼ ì¶œë ¥\n",
    "        for i, result in enumerate(results[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ\n",
    "            print(f\"{i}. {result.page_content[:60]}...\")\n",
    "            print(f\"   ğŸ“‹ ë©”íƒ€ë°ì´í„°: {result.metadata}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\")\n",
    "    \n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) ì°¸ì¡° ë¬¸ì„œ ì—†ì´ ì§ì ‘ ë‹µë³€ì„ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Prompt #ì´ˆì•ˆì€ í•œêµ­ì–´ë˜ë„, í”„ë¡¬í”„íŠ¸ëŠ” ì˜ì–´ë¡œ í•œë‹¤. ê·¸ë¦¬ê³  \n",
    "template = '''Answer the question based only on the following context.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Answer (in í•œêµ­ì–´)]\n",
    "'''\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# ë¬¸ì„œ í¬ë§·íŒ…\n",
    "def format_docs(docs):\n",
    "    return '\\n\\n'.join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5},\n",
    ") #ë²¡í„°ìŠ¤í† ì–´ë¦¬íŠ¸ë¦¬ë²„\n",
    "\n",
    "\n",
    "# Chain êµ¬ì„±\n",
    "rag_chain = (\n",
    "    {'context': retriever | format_docs, 'question': RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Chain ì‹¤í–‰\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "rag_chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) ì°¸ì¡° ë¬¸ì„œë¥¼ ë‹µë³€ê³¼ í•¨ê»˜ ë°˜í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”. ë‘ë²ˆì§¸ëŠ” ragë©´ ë¦¬íŠ¸ë¦¬ë²„ê°€ê²€ìƒ‰í•´ì˜¨ ê²°ê³¼ ì½”ë“œë¥¼ , ì›¹ì´ë©´ ê²€ìƒ‰ê²°ê³¼ ë‹¤ ë³´ì—¬ì£¼ëŠ” ê±¸ ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì£¼ëŠ” ì½”ë“œ\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "def get_context_and_docs(question: str) -> Dict:\n",
    "    \"\"\"ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        question: ê²€ìƒ‰í•  ì§ˆë¬¸\n",
    "\n",
    "    Returns:\n",
    "        Dict: ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸, ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\n",
    "        \"question\": None,  # ì§ˆë¬¸\n",
    "        \"context\": None,   # ë¬¸ì„œ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸\n",
    "        \"source_documents\": None   # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from typing import Dict\n",
    "\n",
    "def get_context_and_docs(question: str) -> Dict:\n",
    "    \"\"\"ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ í•¨ê»˜ ë°˜í™˜\n",
    "    \n",
    "    Args:\n",
    "        question: ê²€ìƒ‰í•  ì§ˆë¬¸\n",
    "\n",
    "    Returns:\n",
    "        Dict: ë¬¸ì„œì™€ í¬ë§·íŒ…ëœ ì»¨í…ìŠ¤íŠ¸, ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "    \"\"\"\n",
    "\n",
    "    # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°\n",
    "    docs = retriever.invoke(question)\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"context\": format_docs(docs),\n",
    "        \"source_documents\": docs\n",
    "    }\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(2) ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” í•¨ìˆ˜`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def prompt_and_generate_answer(input_data: Dict) -> Dict:\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ìƒì„±\n",
    "\n",
    "    Args:\n",
    "        input_data (Dict): ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    Returns:\n",
    "        Dict: ìƒì„±ëœ ë‹µë³€ê³¼ ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "\n",
    "    # LCEL ì²´ì¸ êµ¬ì„± (StrOutputParser ì‚¬ìš©)\n",
    "    answer_chain = None\n",
    "\n",
    "    return {\n",
    "        \"answer\": None,  # ìƒì„±ëœ ë‹µë³€ (answer_chain ê²°ê³¼)\n",
    "        \"source_documents\": None  # ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ (input_dataì—ì„œ ê°€ì ¸ì˜´)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def prompt_and_generate_answer(input_data: Dict) -> Dict:\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ë‹µë³€ì„ ìƒì„±\n",
    "\n",
    "    Args:\n",
    "        input_data (Dict): ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "\n",
    "    Returns:\n",
    "        Dict: ìƒì„±ëœ ë‹µë³€ê³¼ ì†ŒìŠ¤ ë¬¸ì„œ ì •ë³´ê°€ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬\n",
    "    \"\"\"\n",
    "\n",
    "    # LCEL ì²´ì¸ êµ¬ì„±\n",
    "    answer_chain = prompt | llm | StrOutputParser() #ìœ„ì—ì„œ ë‹µë³€ì„ ì—”ì„œì—ì„œ ë¦¬í„´ì„í•´ì¤€ë‹¤. ë¦¬íŠ¸ë¦¬ë²„ê°€í•œê±°ë‘ ì§ì ‘ ì²´ì¸ì—ì„œ ë¦¬í„´í•˜ëŠ”ê±°ë‘ ë‹¤ë¥´ë‹¤.\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer_chain.invoke({\n",
    "            \"context\": input_data['context'], \n",
    "            \"question\": input_data['question']\n",
    "        }),\n",
    "        \"source_documents\": input_data['source_documents']\n",
    "    }\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(3) RAG ì²´ì¸ êµ¬ì„±`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ìŒ ì½”ë“œë¥¼ ì™„ì„±í•˜ì„¸ìš”.\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# Chain êµ¬ì„±\n",
    "rag_chain = (\n",
    "    None(get_context_and_docs) |  # ë¬¸ì„œì™€ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° \n",
    "    {\n",
    "        'response': None(prompt_and_generate_answer), # ë‹µë³€ ìƒì„±\n",
    "        'question': None(),\n",
    "        \"source_documents\": None(\"source_documents\")   # ì†ŒìŠ¤ ë°˜í™˜\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "\n",
    "# Chain êµ¬ì„±\n",
    "rag_chain = (\n",
    "    RunnableLambda(get_context_and_docs) |  # ë¬¸ì„œì™€ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° \n",
    "    {\n",
    "        'response': RunnableLambda(prompt_and_generate_answer), # ë‹µë³€ ìƒì„±\n",
    "        'question': RunnablePassthrough(), #ì´ê²Œì•„ë‹ˆê³  í‹€ë¦°ê±°ê³  itemgetter(\"question\") ì´ ì •ë‹µì´ë‹¤ \n",
    "        \"source_documents\": itemgetter(\"source_documents\")   # ì†ŒìŠ¤ ë°˜í™˜\n",
    "    }\n",
    ")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain ì‹¤í–‰\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "result = rag_chain.invoke(query)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"ë‹µë³€:\", result[\"response\"][\"answer\"])\n",
    "print(\"\\nì°¸ì¡° ë¬¸ì„œ:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) ê²€ìƒ‰ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(1) ê²€ìƒ‰ ë¬¸ì„œì™€ ì§ˆë¬¸ ê°„ì˜ ê´€ë ¨ì„±ì„ í‰ê°€`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²€ìƒ‰ ë¬¸ì„œì˜ ì§ˆë¬¸ ê´€ë ¨ì„± í‰ê°€\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ ë…¼ë¦¬ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”.\n",
    "ë‹¨ê³„ì ìœ¼ë¡œ ì§„í–‰í•˜ë©°, í‰ê°€ê²°ê³¼ì— ëŒ€í•œ ê²€ì¦ì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì¶©ì¡±í•  ê²½ìš° 'Yes'ë¡œ ë‹µë³€í•˜ê³ , ëª¨ë‘ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ 'No'ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "1. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?\n",
    "2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¡œë¶€í„° ë‹µë³€ì— í•„ìš”í•œ ë‚´ìš©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "3. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "\n",
    "'Yes' ë˜ëŠ” 'No'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"),\n",
    "    (\"human\", \"\"\"[ì»¨í…ìŠ¤íŠ¸]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\"\"\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()    # gpt-4.1-mini ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"í‰ê°€ ê²°ê³¼: {relevance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt-4.1 ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "llm_gpt4 = ChatOpenAI(\n",
    "    model='gpt-4.1',\n",
    "    temperature=0.1,\n",
    "    top_p=0.9, \n",
    ")\n",
    "\n",
    "chain = prompt | llm_gpt4 | StrOutputParser()    # gpt-4.1 ëª¨ë¸ ì‚¬ìš©\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")\n",
    "    relevance = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    }).lower()\n",
    "\n",
    "    print(f\"í‰ê°€ ê²°ê³¼: {relevance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ***[ì‹¤ìŠµ] ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì²´ì¸ì„ êµ¬ì¡°í™” ì¶œë ¥ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.*** \n",
    "\n",
    "- pydantic schema ì‚¬ìš©\n",
    "- with_structured_output í•¨ìˆ˜ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>ğŸ’¡ ì •ë‹µ ë³´ê¸°</summary>\n",
    "\n",
    "```python\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "class DocumentRelevance(BaseModel):\n",
    "    \"\"\"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ê²°ê³¼\"\"\"\n",
    "    is_relevant: Literal[\"yes\", \"no\"] = Field(\n",
    "        description=\"ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ì—¬ë¶€\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ”ì§€ ì—¬ë¶€ì— ëŒ€í•œ ì´ìœ \"\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "\n",
    "ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì¶©ì¡±í•  ê²½ìš° 'yes'ë¡œ, ëª¨ë‘ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ 'no'ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "1. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?\n",
    "2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¡œë¶€í„° ë‹µë³€ì— í•„ìš”í•œ ë‚´ìš©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?\"\"\"),\n",
    "    (\"human\", \"\"\"[ì»¨í…ìŠ¤íŠ¸]\n",
    "{context}\n",
    "\n",
    "[ì§ˆë¬¸]\n",
    "{question}\"\"\")\n",
    "])\n",
    "\n",
    "structured_llm = llm.with_structured_output(DocumentRelevance)\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"\\në¬¸ì„œ {i}:\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content}\")\n",
    "    \n",
    "    evaluation = chain.invoke({\n",
    "        \"context\": doc.page_content,\n",
    "        \"question\": query\n",
    "    })\n",
    "    \n",
    "    print(f\"í‰ê°€ ê²°ê³¼: {evaluation.is_relevant}\")\n",
    "    print(f\"ì´ìœ : {evaluation.reasoning}\")\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradio ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤\n",
    "\n",
    "- uv add gradio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vector_store' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m      7\u001b[39m vector_sotre = Chroma(\n\u001b[32m      8\u001b[39m     collection_name=\u001b[33m\"\u001b[39m\u001b[33mhousing_faq_db\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     persist_directory=\u001b[33m\"\u001b[39m\u001b[33m./chroma_db\u001b[39m\u001b[33m\"\u001b[39m, \n\u001b[32m     10\u001b[39m     embedding_function=embeddings,\n\u001b[32m     11\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m retriever = \u001b[43mvector_store\u001b[49m.as_retriever(\n\u001b[32m     16\u001b[39m     search_kwargs={\u001b[33m\"\u001b[39m\u001b[33mk\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m5\u001b[39m},\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\u001b[39;00m\n\u001b[32m     20\u001b[39m query = \u001b[33m\"\u001b[39m\u001b[33mìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'vector_store' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë²¡í„° ì €ì¥ì†Œ ë¡œë“œ\n",
    "vector_sotre = Chroma(\n",
    "    collection_name=\"housing_faq_db\",\n",
    "    persist_directory=\"./chroma_db\", \n",
    "    embedding_function=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "# ê²€ìƒ‰ê¸° ìƒì„± - ìœ ì‚¬ë„ ê¸°ë°˜ ìƒìœ„ 5ê°œ ë¬¸ì„œ ê²€ìƒ‰\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5},\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì§ˆë¬¸\n",
    "query = \"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"\n",
    "\n",
    "results = retriever.invoke(query)\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(\"-\" * 50)\n",
    "    print(result.metadata['keyword'])\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class SearchResult:\n",
    "    context: str\n",
    "    source_documents: Optional[List]\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(\n",
    "            self, \n",
    "            llm: BaseChatModel, \n",
    "            eval_llm: BaseChatModel,\n",
    "            retriever: VectorStoreRetriever\n",
    "        ):\n",
    "        if not llm:\n",
    "            self.llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
    "        else:\n",
    "            self.llm = llm\n",
    "\n",
    "        if not eval_llm:\n",
    "            self.eval_llm = ChatOpenAI(model=\"gpt-4.1\", temperature=0)\n",
    "        else:\n",
    "            self.eval_llm = eval_llm\n",
    "\n",
    "        if not retriever:\n",
    "            raise ValueError(\"ê²€ìƒ‰ê¸°(retriever)ê°€ í•„ìš”í•©ë‹ˆë‹¤.\")\n",
    "        else:\n",
    "            self.retriever = retriever\n",
    "        \n",
    "    def _format_docs(self, docs: List) -> str:\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    def _format_source_documents(self, docs: Optional[List]) -> str:\n",
    "        if not docs:\n",
    "            return \"\\n\\nâ„¹ï¸ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"\n",
    "        \n",
    "        formatted_docs = []\n",
    "        for i, doc in enumerate(docs, 1):\n",
    "            metadata = doc.metadata if hasattr(doc, 'metadata') else {}\n",
    "            source_info = []\n",
    "            \n",
    "            if 'question_id' in metadata:\n",
    "                source_info.append(f\"ID: {metadata['question_id']}\")\n",
    "            if 'keyword' in metadata:\n",
    "                source_info.append(f\"í‚¤ì›Œë“œ: {metadata['keyword']}\")\n",
    "            if 'summary' in metadata:\n",
    "                source_info.append(f\"ìš”ì•½: {metadata['summary']}\")\n",
    "                \n",
    "            formatted_docs.append(\n",
    "                f\"ğŸ“š ì°¸ì¡° ë¬¸ì„œ {i}\\n\"\n",
    "                f\"â€¢ {' | '.join(source_info) if source_info else 'ì¶œì²˜ ì •ë³´ ì—†ìŒ'}\\n\"\n",
    "                f\"â€¢ ë‚´ìš©: {doc.page_content}\"\n",
    "            )\n",
    "        \n",
    "        return \"\\n\\n\" + \"\\n\\n\".join(formatted_docs)\n",
    "    \n",
    "    def _check_relevance(self, docs: List, question: str) -> List:\n",
    "        \"\"\"ë¬¸ì„œì˜ ê´€ë ¨ì„± í™•ì¸\"\"\"\n",
    "\n",
    "        relevant_docs = []\n",
    "\n",
    "        if not docs:\n",
    "            return relevant_docs\n",
    "            \n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.\n",
    "\n",
    "        ë‹¤ìŒ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ ì´ìƒì„ ì¶©ì¡±í•  ê²½ìš° 'Yes'ë¡œ ë‹µë³€í•˜ê³ , ëª¨ë‘ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë©´ 'No'ë¡œ ë‹µë³€í•˜ì„¸ìš”:\n",
    "\n",
    "        1. ì»¨í…ìŠ¤íŠ¸ê°€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ”ë° í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ í¬í•¨í•˜ê³  ìˆëŠ”ê°€?\n",
    "        2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¡œë¶€í„° ë‹µë³€ì— í•„ìš”í•œ ë‚´ìš©ì„ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ”ê°€?\n",
    "\n",
    "        'Yes' ë˜ëŠ” 'No'ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\"\"\"),\n",
    "            (\"human\", \"\"\"[ì»¨í…ìŠ¤íŠ¸]\n",
    "        {context}\n",
    "\n",
    "        [ì§ˆë¬¸]\n",
    "        {question}\"\"\")\n",
    "        ])\n",
    "        \n",
    "        chain = prompt | self.eval_llm | StrOutputParser()\n",
    "\n",
    "        for doc in docs:\n",
    "            result = chain.invoke({\n",
    "                \"context\": doc.page_content,\n",
    "                \"question\": question\n",
    "            }).lower()\n",
    "\n",
    "            print(f\"ë¬¸ì„œ {doc.metadata['question_id']} ê´€ë ¨ì„± í™•ì¸ ê²°ê³¼: {result}\")\n",
    "            print(f\"ë¬¸ì„œ {doc.metadata['question_id']} ë‚´ìš©:\")\n",
    "            print(doc.page_content)\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "            if \"yes\" in result:\n",
    "                relevant_docs.append(doc)\n",
    "            \n",
    "        return relevant_docs\n",
    "    \n",
    "    def search_documents(self, question: str) -> SearchResult:\n",
    "        try:\n",
    "            docs = retriever.invoke(question)\n",
    "            print(f\"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}\")\n",
    "            relevant_docs = self._check_relevance(docs, question) \n",
    "            print(f\"ê´€ë ¨ ë¬¸ì„œ ê°œìˆ˜: {len(relevant_docs)}\")\n",
    "            \n",
    "            return SearchResult(\n",
    "                context=self._format_docs(relevant_docs) if relevant_docs else \"ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\",\n",
    "                source_documents=relevant_docs,\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "            return SearchResult(\n",
    "                context=\"ë¬¸ì„œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\",\n",
    "                source_documents=None,\n",
    "            )\n",
    "    \n",
    "    def generate_answer(self, message: str, history: List) -> str:\n",
    "        # ë¬¸ì„œ ê²€ìƒ‰\n",
    "        search_result = self.search_documents(message)\n",
    "        \n",
    "        if not search_result.source_documents:\n",
    "            return \"ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ë‹µë³€í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê² ìŠµë‹ˆê¹Œ?\"\n",
    "                    \n",
    "        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"\"\"ë‹¤ìŒ ì§€ì¹¨ì„ ë”°ë¼ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”:\n",
    "            1. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "            2. ë¬¸ì„œì— ëª…í™•í•œ ê·¼ê±°ê°€ ì—†ëŠ” ë‚´ìš©ì€ \"ê·¼ê±° ì—†ìŒ\"ì´ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "            3. ë‹µë³€í•˜ê¸° ì–´ë ¤ìš´ ì§ˆë¬¸ì€ \"ì˜ ëª¨ë¥´ê² ìŠµë‹ˆë‹¤\"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n",
    "            4. ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.\"\"\"),\n",
    "            (\"human\", \"ë¬¸ì„œë“¤:\\n{context}\\n\\nì§ˆë¬¸: {question}\")\n",
    "        ])\n",
    "        \n",
    "        # RAG Chain êµ¬ì„±\n",
    "        chain = prompt | self.llm | StrOutputParser()\n",
    "        \n",
    "        try:\n",
    "            # ë‹µë³€ ìƒì„±\n",
    "            answer = chain.invoke({\n",
    "                \"context\": search_result.context,\n",
    "                \"question\": message\n",
    "            })\n",
    "            \n",
    "            # ì°¸ì¡° ë¬¸ì„œ í¬ë§·íŒ… ì¶”ê°€\n",
    "            sources = self._format_source_documents(search_result.source_documents)\n",
    "            return f\"{answer}\\n{sources}\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\"\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "\n",
    "rag_system = RAGSystem(\n",
    "    llm=ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0),   \n",
    "    eval_llm=ChatOpenAI(model=\"gpt-4.1\", temperature=0),\n",
    "    retriever=vector_store.as_retriever(search_kwargs={\"k\": 2})\n",
    ")\n",
    "\n",
    "demo = gr.ChatInterface(\n",
    "    fn=rag_system.generate_answer,\n",
    "    title=\"RAG QA ì‹œìŠ¤í…œ\",\n",
    "    description=\"\"\"\n",
    "    ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    ëª¨ë“  ë‹µë³€ì—ëŠ” ì°¸ì¡°í•œ ë¬¸ì„œì˜ ì¶œì²˜ê°€ í‘œì‹œë©ë‹ˆë‹¤.\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\"ìˆ˜ì›ì‹œì˜ ì£¼íƒê±´ì„¤ì§€ì—­ì€ ì–´ë””ì— í•´ë‹¹í•˜ë‚˜ìš”?\"],\n",
    "        [\"ë¬´ì£¼íƒ ì„¸ëŒ€ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"],\n",
    "        [\"2ìˆœìœ„ë¡œ ë‹¹ì²¨ëœ ì‚¬ëŒì´ ì²­ì•½í†µì¥ì„ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë‚˜ìš”?\"],\n",
    "    ],\n",
    "    analytics_enabled=False,\n",
    ")\n",
    "\n",
    "# ë°ëª¨ ì‹¤í–‰\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì¢…ë£Œ\n",
    "demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **[í”„ë¡œì íŠ¸ ì‹¤ìŠµ] ì£¼íƒì²­ì•½ FAQ ì‹œìŠ¤í…œ êµ¬í˜„**\n",
    "\n",
    "ì´ì „ ì½”ë“œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì£¼íƒì²­ì•½ FAQ ì‹œìŠ¤í…œì„ ë‹¤ìŒ ìš”êµ¬ì‚¬í•­ì— ë§ì¶° ê°œì„ í•©ë‹ˆë‹¤. \n",
    "\n",
    "- ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€ (ë‹µë³€ì´ ë¶ˆì¶©ë¶„í•œ ê²½ìš° ì˜ˆì™¸ ì²˜ë¦¬)\n",
    "- ê´€ë ¨ì„± ë†’ì€ FAQ ë¬¸ì„œ ê²€ìƒ‰ (ì„ë² ë”© ëª¨ë¸, ì²­í¬ í¬ê¸°, ë²¡í„° ê²€ìƒ‰ ë°©ë²• ë“±)\n",
    "\n",
    "ì œì•½ ì¡°ê±´:\n",
    "- Gradio ChatInterface ì‚¬ìš©\n",
    "- RAG êµ¬ì¡° ìœ ì§€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—¬ê¸°ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "004-llm-agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
